{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55f6f2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during data cleaning: File not found: your_data_file.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "class DataCleaningPipeline:\n",
    "    \"\"\"A flexible data cleaning pipeline for Excel and CSV files with multiple sheets.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cleaning_functions = {\n",
    "            'remove_empty_rows_cols': self.remove_empty_rows_cols,\n",
    "            'standardize_column_names': self.standardize_column_names,\n",
    "            'convert_data_types': self.convert_data_types,\n",
    "            'handle_missing_values': self.handle_missing_values,\n",
    "            'remove_duplicates': self.remove_duplicates,\n",
    "            'clean_text_fields': self.clean_text_fields,\n",
    "            'handle_outliers': self.handle_outliers,\n",
    "            'extract_datetime_features': self.extract_datetime_features\n",
    "        }\n",
    "    \n",
    "    def process_file(self, file_path, sheet_name=None, cleaning_steps=None):\n",
    "        \"\"\"\n",
    "        Process a file (Excel or CSV) with the specified cleaning steps.\n",
    "        \n",
    "        Parameters:\n",
    "        file_path (str): Path to the file\n",
    "        sheet_name (str or list): Specific sheet name(s) to process (for Excel files)\n",
    "        cleaning_steps (list): List of cleaning steps to apply\n",
    "        \n",
    "        Returns:\n",
    "        dict: Dictionary of cleaned DataFrames\n",
    "        \"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "        # Determine file type and read data\n",
    "        if file_path.suffix.lower() in ['.xlsx', '.xls']:\n",
    "            return self.process_excel(file_path, sheet_name, cleaning_steps)\n",
    "        elif file_path.suffix.lower() == '.csv':\n",
    "            return self.process_csv(file_path, cleaning_steps)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_path.suffix}\")\n",
    "    \n",
    "    def process_excel(self, file_path, sheet_name=None, cleaning_steps=None):\n",
    "        \"\"\"Process an Excel file with multiple sheets.\"\"\"\n",
    "        # Read all sheets or specific sheets\n",
    "        if sheet_name is None:\n",
    "            sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "        else:\n",
    "            if isinstance(sheet_name, str):\n",
    "                sheet_name = [sheet_name]\n",
    "            sheets = {}\n",
    "            for sheet in sheet_name:\n",
    "                try:\n",
    "                    sheets[sheet] = pd.read_excel(file_path, sheet_name=sheet)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not read sheet '{sheet}': {e}\")\n",
    "        \n",
    "        # Apply cleaning steps to each sheet\n",
    "        cleaned_sheets = {}\n",
    "        for name, df in sheets.items():\n",
    "            print(f\"Processing sheet: {name}\")\n",
    "            cleaned_df = self.apply_cleaning_steps(df, cleaning_steps)\n",
    "            cleaned_sheets[name] = cleaned_df\n",
    "        \n",
    "        return cleaned_sheets\n",
    "    \n",
    "    def process_csv(self, file_path, cleaning_steps=None):\n",
    "        \"\"\"Process a CSV file.\"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        cleaned_df = self.apply_cleaning_steps(df, cleaning_steps)\n",
    "        return {'data': cleaned_df}\n",
    "    \n",
    "    def apply_cleaning_steps(self, df, cleaning_steps):\n",
    "        \"\"\"Apply the specified cleaning steps to a DataFrame.\"\"\"\n",
    "        if cleaning_steps is None:\n",
    "            # Apply all cleaning steps if none specified\n",
    "            cleaning_steps = list(self.cleaning_functions.keys())\n",
    "        \n",
    "        for step in cleaning_steps:\n",
    "            if step in self.cleaning_functions:\n",
    "                print(f\"  Applying step: {step}\")\n",
    "                df = self.cleaning_functions[step](df)\n",
    "            else:\n",
    "                print(f\"  Warning: Unknown cleaning step '{step}'\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def remove_empty_rows_cols(self, df):\n",
    "        \"\"\"Remove completely empty rows and columns.\"\"\"\n",
    "        # Remove rows where all values are missing\n",
    "        df = df.dropna(how='all')\n",
    "        \n",
    "        # Remove columns where all values are missing\n",
    "        df = df.dropna(axis=1, how='all')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def standardize_column_names(self, df):\n",
    "        \"\"\"Standardize column names to snake_case.\"\"\"\n",
    "        new_columns = []\n",
    "        for col in df.columns:\n",
    "            # Handle non-string column names\n",
    "            if not isinstance(col, str):\n",
    "                col = str(col)\n",
    "            \n",
    "            # Remove special characters and extra spaces\n",
    "            col = re.sub(r'[^a-zA-Z0-9\\s]', ' ', col)\n",
    "            col = re.sub(r'\\s+', ' ', col).strip()\n",
    "            \n",
    "            # Convert to snake_case\n",
    "            col = col.replace(' ', '_').lower()\n",
    "            \n",
    "            new_columns.append(col)\n",
    "        \n",
    "        df.columns = new_columns\n",
    "        return df\n",
    "    \n",
    "    def convert_data_types(self, df):\n",
    "        \"\"\"Convert data types appropriately.\"\"\"\n",
    "        for col in df.columns:\n",
    "            # Skip if all values are missing\n",
    "            if df[col].isna().all():\n",
    "                continue\n",
    "            \n",
    "            # Try to convert to numeric\n",
    "            try:\n",
    "                # Check if the column contains numeric values stored as strings\n",
    "                if df[col].dtype == 'object':\n",
    "                    # Try to convert to numeric, handling commas and other non-numeric characters\n",
    "                    converted = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')\n",
    "                    # Only convert if we successfully converted most values\n",
    "                    if converted.notna().mean() > 0.8:  # If more than 80% converted successfully\n",
    "                        df[col] = converted\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Try to convert to datetime\n",
    "            try:\n",
    "                if df[col].dtype == 'object':\n",
    "                    # Try to convert to datetime\n",
    "                    converted = pd.to_datetime(df[col], errors='coerce')\n",
    "                    # Only convert if we successfully converted most values\n",
    "                    if converted.notna().mean() > 0.8:  # If more than 80% converted successfully\n",
    "                        df[col] = converted\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def handle_missing_values(self, df):\n",
    "        \"\"\"Handle missing values based on data type.\"\"\"\n",
    "        for col in df.columns:\n",
    "            if df[col].isna().any():\n",
    "                if df[col].dtype in ['int64', 'float64']:\n",
    "                    # For numeric columns, fill with median\n",
    "                    df[col] = df[col].fillna(df[col].median())\n",
    "                elif df[col].dtype == 'object':\n",
    "                    # For categorical columns, fill with mode\n",
    "                    df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown')\n",
    "                elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "                    # For datetime columns, fill with the most recent date\n",
    "                    df[col] = df[col].fillna(df[col].max())\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def remove_duplicates(self, df):\n",
    "        \"\"\"Remove duplicate rows.\"\"\"\n",
    "        return df.drop_duplicates()\n",
    "    \n",
    "    def clean_text_fields(self, df):\n",
    "        \"\"\"Clean text fields by removing extra whitespace and standardizing.\"\"\"\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                # Remove leading/trailing whitespace\n",
    "                df[col] = df[col].astype(str).str.strip()\n",
    "                \n",
    "                # Replace multiple spaces with single space\n",
    "                df[col] = df[col].str.replace(r'\\s+', ' ', regex=True)\n",
    "                \n",
    "                # Standardize case (title case for proper names, lower for others)\n",
    "                if any(keyword in col.lower() for keyword in ['name', 'title', 'category']):\n",
    "                    df[col] = df[col].str.title()\n",
    "                else:\n",
    "                    df[col] = df[col].str.lower()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def handle_outliers(self, df):\n",
    "        \"\"\"Handle outliers in numeric columns using IQR method.\"\"\"\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            # Define bounds\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            # Cap outliers\n",
    "            df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])\n",
    "            df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def extract_datetime_features(self, df):\n",
    "        \"\"\"Extract features from datetime columns.\"\"\"\n",
    "        datetime_cols = df.select_dtypes(include=['datetime64[ns]']).columns\n",
    "        \n",
    "        for col in datetime_cols:\n",
    "            # Extract year, month, day, etc.\n",
    "            df[f'{col}_year'] = df[col].dt.year\n",
    "            df[f'{col}_month'] = df[col].dt.month\n",
    "            df[f'{col}_day'] = df[col].dt.day\n",
    "            df[f'{col}_dayofweek'] = df[col].dt.dayofweek\n",
    "            df[f'{col}_quarter'] = df[col].dt.quarter\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def save_cleaned_data(self, data_dict, output_path, format='excel'):\n",
    "        \"\"\"Save the cleaned data to file(s).\"\"\"\n",
    "        if format == 'excel':\n",
    "            with pd.ExcelWriter(output_path) as writer:\n",
    "                for sheet_name, df in data_dict.items():\n",
    "                    df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        elif format == 'csv':\n",
    "            # Create a directory for CSV files\n",
    "            output_dir = Path(output_path).parent / 'cleaned_data'\n",
    "            output_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            for sheet_name, df in data_dict.items():\n",
    "                safe_name = re.sub(r'[^a-zA-Z0-9]', '_', sheet_name)\n",
    "                df.to_csv(output_dir / f'{safe_name}.csv', index=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported output format: {format}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the pipeline\n",
    "    pipeline = DataCleaningPipeline()\n",
    "    \n",
    "    # Define the cleaning steps to apply\n",
    "    cleaning_steps = [\n",
    "        'remove_empty_rows_cols',\n",
    "        'standardize_column_names',\n",
    "        'convert_data_types',\n",
    "        'handle_missing_values',\n",
    "        'remove_duplicates',\n",
    "        'clean_text_fields',\n",
    "        'handle_outliers'\n",
    "    ]\n",
    "    \n",
    "    # Process a file\n",
    "    try:\n",
    "        # For Excel files with multiple sheets\n",
    "        cleaned_data = pipeline.process_file(\n",
    "            file_path=\"your_data_file.xlsx\",\n",
    "            cleaning_steps=cleaning_steps\n",
    "        )\n",
    "        \n",
    "        # Save the cleaned data\n",
    "        pipeline.save_cleaned_data(cleaned_data, \"cleaned_data.xlsx\")\n",
    "        \n",
    "        print(\"Data cleaning completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during data cleaning: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d05080be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sheet: Index\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: Request numbers\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: Action on requests\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: Response times\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: Charges\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: Internal review\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: Section 48 primary\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: Section 48 response time\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: Section 48 internal review\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: FOI Summary of salary & admin c\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: IPS Summary of salary & admin c\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: FOI non-labour costs\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: IPS non-labour costs\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: Practical refusal\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: Exemptions\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: Staff years and costs by level\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: Agency comments\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: Requests top 20\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: Determined top 20\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: Charges top 20\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n",
      "Processing sheet: Disclosure Log\n",
      "  Warning: Unknown cleaning step 'standardize_column_name'\n",
      "  Applying step: convert_data_types\n",
      "  Applying step: handle_missing_values\n",
      "  Warning: Unknown cleaning step 'remove-dublicate'\n",
      "  Applying step: clean_text_fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\Aseem\\AppData\\Local\\Temp\\ipykernel_10472\\3648932258.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted = pd.to_datetime(df[col], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "pipline = DataCleaningPipeline()\n",
    "cleaning_steps = ['standardize_column_name', 'convert_data_types', 'handle_missing_values',\n",
    "                  'remove-dublicate', 'clean_text_fields']\n",
    "\n",
    "cleaned_data = pipeline.process_file('../GovHack-Team-ASTRA/Datasets/same_dataset/agency-foi-data-2023-24.xlsx', cleaning_steps=cleaning_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b28ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.save_cleaned_data(cleaned_data, 'testing.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hit140env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
